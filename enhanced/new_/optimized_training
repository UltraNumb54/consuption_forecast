import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# === –ö–û–ù–°–¢–ê–ù–¢–´ ===
OPTIMIZED_DATA_FILE = 'optimized_processed_energy_data.csv'
MODEL_PATH = 'optimized_energy_model.cbm'

def mape_scorer(y_true, y_pred):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π scorer –¥–ª—è MAPE"""
    return -mean_absolute_percentage_error(y_true, y_pred)

def get_optimized_features(df):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô —Å–ø–∏—Å–æ–∫ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    # –û–°–ù–û–í–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º
    core_features = [
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è
        'consumption_lag_1', 'consumption_lag_2', 'consumption_lag_3',
        'consumption_lag_24', 'consumption_lag_48', 'consumption_lag_168',
        'consumption_rolling_mean_24', 'consumption_rolling_mean_168',
        'consumption_ewm_24',
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–∞—Å–∞
        'hour_sin', 'hour_cos',
        
        # –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        'is_weekend', 'is_holiday', 'is_working_day', 'is_working_weekend',
        
        # –ü–æ–≥–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        'temperature', 'dew_point', 'pressure_station', 'humidity',
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        'year', 'season', 'hour', 'month', 'dayofweek'
    ]
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    existing_features = [feature for feature in core_features if feature in df.columns]
    
    print("–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
    for i, feature in enumerate(existing_features):
        print(f"{i+1:2d}. {feature}")
    
    return existing_features

def train_optimized_model():
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"""
    print("=== –û–ë–£–ß–ï–ù–ò–ï –ù–ê –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–ê–• ===")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv(OPTIMIZED_DATA_FILE)
    df['datetime'] = pd.to_datetime(df['datetime'])
    df = df.sort_values('datetime')

    print(f"–î–∞–Ω–Ω—ã–µ: {len(df)} –∑–∞–ø–∏—Å–µ–π")
    print(f"–ü–µ—Ä–∏–æ–¥: {df['datetime'].min()} - {df['datetime'].max()}")
    
    # –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    feature_columns = get_optimized_features(df)
    
    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    target_column = 'consumption'
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º X –∏ y
    X = df[feature_columns]
    y = df[target_column]
    
    print(f"X shape: {X.shape}, y shape: {y.shape}")

    # –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    categorical_features = [
        'season', 'is_weekend', 'is_holiday', 'is_working_day', 'is_working_weekend',
        'year', 'month', 'dayofweek', 'hour'
    ]
    categorical_features = [col for col in categorical_features if col in feature_columns]
    
    print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {categorical_features}")

    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    tscv = TimeSeriesSplit(n_splits=3)  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏

    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ—Ç–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    param_grid = {
        'iterations': [300, 500],
        'learning_rate': [0.05, 0.1],
        'depth': [6, 8],
        'l2_leaf_reg': [3, 5],
        'random_seed': [42]
    }

    # –ú–æ–¥–µ–ª—å
    model = CatBoostRegressor(
        loss_function='MAPE',
        cat_features=categorical_features,
        verbose=False
    )

    # –ü–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    print("\nüîç –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=make_scorer(mape_scorer, greater_is_better=False),
        cv=tscv,
        n_jobs=-1,
        verbose=1
    )

    grid_search.fit(X, y)

    print(f"\nüèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
    print(f"üìà –õ—É—á—à–∏–π MAPE: {-grid_search.best_score_:.4f}")

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å early stopping
    split_idx = int(len(X) * 0.8)
    X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]

    print("\nüöÄ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
    best_model = CatBoostRegressor(
        **grid_search.best_params_,
        loss_function='MAPE',
        eval_metric='MAPE',
        cat_features=categorical_features,
        early_stopping_rounds=20,
        use_best_model=True,
        verbose=50
    )

    best_model.fit(X_train, y_train, eval_set=(X_val, y_val))

    # –û—Ü–µ–Ω–∫–∞
    y_pred = best_model.predict(X)
    mae = mean_absolute_error(y, y_pred)
    mape = mean_absolute_percentage_error(y, y_pred) * 100
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   MAE: {mae:.3f}")
    print(f"   MAPE: {mape:.2f}%")

    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print(f"\n=== –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í ===")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.get_feature_importance()
    }).sort_values('importance', ascending=False)
    
    for i, row in feature_importance.iterrows():
        print(f"{i+1:2d}. {row['feature']:<30} : {row['importance']:>8.2f}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    best_model.save_model(MODEL_PATH)
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")

    return best_model, feature_columns

if __name__ == "__main__":
    model, features = train_optimized_model()
    print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
