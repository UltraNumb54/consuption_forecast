# src/model_training.py

import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
import os
import logging

# Пути к модели
MODELS_DIR = "../models"
MODEL_PATH = os.path.join(MODELS_DIR, "latest_model.cbm")
MODEL_PATH_PREVIOUS = os.path.join(MODELS_DIR, "previous_model.cbm")

def train_model(df_for_training, feature_columns):
    """
    Обучает модель на переданном датасете.
    """
    logging.info("=== НАЧАЛО ОБУЧЕНИЯ МОДЕЛИ ===")
    X = df_for_training[feature_columns]
    y = df_for_training['consumption']

    # Определение категориальных признаков
    categorical_features = [
        'hour', 'dayofweek', 'month', 'week_of_year',
        'is_holiday', 'is_working_weekend', 'is_regular_weekend', 'is_working_day',
        'is_weekend_or_holiday', 'is_weekend',
        'is_winter', 'is_spring', 'is_summer', 'is_autumn'
    ]
    categorical_features = [col for col in categorical_features if col in X.columns]

    # Разделение на train/val для early stopping
    split_idx = int(len(X) * 0.8)
    X_train_final = X.iloc[:split_idx]
    y_train_final = y.iloc[:split_idx]
    X_val_final = X.iloc[split_idx:]
    y_val_final = y.iloc[split_idx:]

    # Параметры модели (можно настроить или использовать GridSearch как в оригинале)
    model_params = {
        'iterations': 500,
        'learning_rate': 0.05,
        'depth': 6,
        'l2_leaf_reg': 3,
        'loss_function': 'MAPE',
        'eval_metric': 'MAPE',
        'cat_features': categorical_features,
        'early_stopping_rounds': 30,
        'use_best_model': True,
        'verbose': 50,
        'random_seed': 42
    }

    model = CatBoostRegressor(**model_params)

    logging.info("Обучение модели...")
    model.fit(
        X_train_final, y_train_final,
        eval_set=(X_val_final, y_val_final)
    )

    # Оценка на валидационной выборке
    y_pred_val = model.predict(X_val_final)
    mae_val = mean_absolute_error(y_val_final, y_pred_val)
    mape_val = mean_absolute_percentage_error(y_val_final, y_pred_val) * 100

    logging.info(f"MAE на валидации: {mae_val:.3f}")
    logging.info(f"MAPE на валидации: {mape_val:.2f}%")

    logging.info("=== ОБУЧЕНИЕ МОДЕЛИ ЗАВЕРШЕНО ===")
    return model

def compare_and_save_model(new_model, new_score, feature_columns):
    """
    Сравнивает новую модель с текущей лучшей.
    Если новая лучше, сохраняет её как лучшую.
    """
    current_model_path = MODEL_PATH
    current_score = float('inf') # Предполагаем, что если модели нет, то новая всегда лучше
    current_model = None

    if os.path.exists(current_model_path):
        logging.info(f"Загрузка текущей лучшей модели для сравнения из {current_model_path}")
        current_model = CatBoostRegressor()
        current_model.load_model(current_model_path)
        # Оценка текущей модели на новом датасете (или на той же валидации, если доступна)
        # Здесь мы оцениваем новую метрику (MAPE на валидации) для новой и старой модели
        # Для простоты, сравниваем по MAPE_val, который мы получили при обучении new_model
        # Если у нас есть валидационный датасет для оценки, используем его
        # В данном случае, new_score - это MAPE_val новой модели
        # Проверим, есть ли у текущей модели информация о feature_names для совместимости
        if set(current_model.feature_names_) != set(feature_columns):
             logging.warning("Колонки признаков новой и старой модели не совпадают. Сохраняем новую как лучшую.")
             current_score = float('inf') # Считаем текущую модель "плохой"
        else:
             # Текущая модель может быть оценена на тех же данных, что и новая
             # Но это не всегда корректно. Лучше оценивать на отложенной выборке.
             # В данном упрощённом примере, мы используем MAPE_val как метрику для сравнения.
             # Предположим, что new_score - это MAPE_val новой модели.
             # Мы не можем легко оценить старую модель на новых данных без валидационного сета.
             # Поэтому, если новая модель обучалась с лучшими параметрами или на более свежих данных,
             # она может быть предпочтительнее даже с немного худшей MAPE_val.
             # Для простоты, будем сравнивать MAPE_val.
             # Лучше использовать отложенную тестовую выборку для финального сравнения.
             # Здесь просто используем MAPE_val.
             # Для реального продакшна, нужен отдельный тестовый сет для оценки.
             # current_score = ... # Нужно оценить current_model на валидационном сете
             # Так как у нас нет отдельного валидационного сета для оценки старой модели,
             # мы просто сравниваем new_score (MAPE_val новой модели) с условной "бесконечностью"
             # если старой модели нет, или с её предыдущей MAPE_val (если мы сохранили её).
             # Предположим, что new_score - это MAPE_val новой модели.
             # Мы не можем получить MAPE_val старой модели без её оценки.
             # Поэтому, если старая модель существует, мы её оцениваем заново или используем сохранённую метрику.
             # В реальности, нужно хранить не только модель, но и её метрики (например, в JSON).
             # Для упрощения, предположим, что если старая модель есть, её MAPE_val = 100 (плохо)
             # Это не реалистично, но позволяет заменить на новую.
             # Лучше: при сохранении модели, сохранять и её MAPE_val.
             # model_info_path = current_model_path.replace('.cbm', '_info.json')
             # if os.path.exists(model_info_path):
             #     with open(model_info_path, 'r') as f:
             #         old_info = json.load(f)
             #     current_score = old_info.get('mape_val', float('inf'))
             # else:
             #     current_score = float('inf') # Если нет инфо, считаем старую плохой
             current_score = float('inf') # Упрощение: если модель есть, но инфо нет, считаем новую лучше
             # Правильный способ: при обучении, сохранять MAPE_val в файл рядом с моделью.
             # Тогда можно будет сравнить.
             # Например, сохранить model_info = {'mape_val': mape_val, 'features': list(feature_columns), ...}
             # Тогда при сравнении, загружаем инфо старой модели и сравниваем mape_val.

    # Простое сравнение: если новая модель имеет лучший (меньший) MAPE_val
    if new_score < current_score:
        logging.info("Новая модель лучше текущей. Сохраняем как лучшую.")
        if os.path.exists(current_model_path):
            # Резервное копирование старой лучшей модели
            os.rename(current_model_path, MODEL_PATH_PREVIOUS)
            logging.info(f"Старая лучшая модель перемещена в {MODEL_PATH_PREVIOUS}")
        new_model.save_model(current_model_path)
        # Сохранение информации о модели (рекомендуется)
        # import json
        # model_info = {'mape_val': new_score, 'features': feature_columns, 'training_date': datetime.now().isoformat()}
        # info_path = current_model_path.replace('.cbm', '_info.json')
        # with open(info_path, 'w') as f:
        #     json.dump(model_info, f)
        logging.info(f"Новая лучшая модель сохранена в {current_model_path}")
        return True # Модель обновлена
    else:
        logging.info("Текущая модель лучше новой. Сохранение отменено.")
        return False # Модель не обновлена

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    # Этот файл должен быть запущен из pipeline_runner.py, передав ему обучаемый датасет
    # и список feature_columns.
    # Пример:
    # from preprocess import main_preprocess
    # from data_ingestion import load_consumption_data, load_weather_data, load_calendar_data
    # df_con = load_consumption_data()
    # df_w = load_weather_data()
    # df_cal = load_calendar_data()
    # df_processed = main_preprocess(df_con, df_w, df_cal)
    # # Допустим, df_processed - это df_for_training
    # feature_cols = [col for col in df_processed.columns if col != 'consumption' and col != 'datetime']
    # model = train_model(df_processed, feature_cols)
    # mape_val = ... # Значение MAPE, полученное при обучении
    # compare_and_save_model(model, mape_val, feature_cols)
