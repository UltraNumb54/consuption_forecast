import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# === –ö–û–ù–°–¢–ê–ù–¢–´ ===
MODEL_PATH = 'improved_energy_model.cbm'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
PROCESSED_DATA_FILE = 'processed_energy_data.csv'  # –í—Å–µ –¥–∞–Ω–Ω—ã–µ
TEST_START_DATE = '2025-09-08'  # –¢–µ—Å—Ç —Å 8 —Å–µ–Ω—Ç—è–±—Ä—è
TEST_DAYS = 3
HISTORICAL_WINDOW_DAYS = 180  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º—É–º 180 –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏ –≤–º–µ—Å—Ç–æ 6 –ª–µ—Ç

def load_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    model = CatBoostRegressor()
    model.load_model(MODEL_PATH)
    return model

def get_model_explanation():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("=== –û–ë–™–Ø–°–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò ===")
    
    model = load_model()
    
    # –ó–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    df = pd.read_csv(PROCESSED_DATA_FILE)
    df['datetime'] = pd.to_datetime(df['datetime'])
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    numerical_features = [
        'hour', 'dayofweek', 'month',
        'temperature', 'humidity', 'wind_speed',
        'consumption_lag_1', 'consumption_lag_2', 'consumption_lag_3',
        'consumption_lag_24', 'consumption_lag_48', 'consumption_lag_168',
        'consumption_rolling_mean_3', 'consumption_rolling_mean_6',
        'consumption_rolling_mean_12', 'consumption_rolling_mean_24',
        'consumption_rolling_std_3', 'consumption_rolling_std_6',
        'consumption_rolling_std_12', 'consumption_rolling_std_24'
    ]
    
    categorical_features = [
        'is_holiday', 'is_working_weekend', 'is_regular_weekend',
        'is_working_day', 'is_weekend_or_holiday', 'is_weekend'
    ]
    
    feature_columns = numerical_features + categorical_features
    
    # –ü–æ–ª—É—á–∏–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = model.get_feature_importance()
    feature_names = feature_columns  # –∏–ª–∏ model.feature_names_ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
    
    print("–¢–û–ü-15 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    importance_df = pd.DataFrame({
        'feature': feature_names[:len(feature_importance)],
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    for i, row in importance_df.head(15).iterrows():
        print(f"{i+1:2d}. {row['feature']}: {row['importance']:.2f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plt.figure(figsize=(12, 8))
    top_features = importance_df.head(15)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
    plt.title('–¢–æ–ø-15 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–æ–¥–µ–ª–∏')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('feature_importance_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return importance_df

def prepare_features_with_explanation(df_historical, prediction_datetime, weather_data):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º —Ä–µ—à–µ–Ω–∏–π"""
    print(f"\n--- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {prediction_datetime} ---")
    
    features = {
        'hour': prediction_datetime.hour,
        'dayofweek': prediction_datetime.weekday(),
        'month': prediction_datetime.month,
    }
    print(f"–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: —á–∞—Å={features['hour']}, –¥–µ–Ω—å_–Ω–µ–¥–µ–ª–∏={features['dayofweek']}, –º–µ—Å—è—Ü={features['month']}")
    
    # –ü–æ–≥–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    features.update({
        'temperature': weather_data.get('temperature', 10.0),
        'humidity': weather_data.get('humidity', 70.0),
        'wind_speed': weather_data.get('wind_speed', 3.0),
    })
    print(f"–ü–æ–≥–æ–¥–∞: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞={features['temperature']}, –≤–ª–∞–∂–Ω–æ—Å—Ç—å={features['humidity']}, –≤–µ—Ç–µ—Ä={features['wind_speed']}")
    
    # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º
    print("–õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
    for lag in [1, 2, 3, 24, 48, 168]:
        lag_time = prediction_datetime - timedelta(hours=lag)
        lag_data = df_historical[df_historical['datetime'] == lag_time]
        if len(lag_data) > 0:
            features[f'consumption_lag_{lag}'] = lag_data['consumption'].iloc[0]
            print(f"   lag_{lag}: {features[f'consumption_lag_{lag}']:.1f} –ú–í—Ç (–≤ {lag_time})")
        else:
            features[f'consumption_lag_{lag}'] = df_historical['consumption'].median()
            print(f"   lag_{lag}: –º–µ–¥–∏–∞–Ω–∞ {features[f'consumption_lag_{lag}']:.1f} –ú–í—Ç (–¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç)")
    
    # –°–∫–æ–ª—å–∑—è—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    print("–°–∫–æ–ª—å–∑—è—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
    for window in [3, 6, 12, 24]:
        recent_data = df_historical[
            df_historical['datetime'] < prediction_datetime
        ].tail(window)
        
        if len(recent_data) >= min(3, window):
            mean_val = recent_data['consumption'].mean()
            std_val = recent_data['consumption'].std()
            features[f'consumption_rolling_mean_{window}'] = mean_val
            features[f'consumption_rolling_std_{window}'] = std_val
            print(f"   rolling_mean_{window}: {mean_val:.1f} –ú–í—Ç (std={std_val:.1f})")
        else:
            mean_val = df_historical['consumption'].mean()
            std_val = df_historical['consumption'].std()
            features[f'consumption_rolling_mean_{window}'] = mean_val
            features[f'consumption_rolling_std_{window}'] = std_val
            print(f"   rolling_mean_{window}: –æ–±—â–∞—è —Å—Ä–µ–¥–Ω—è—è {mean_val:.1f} –ú–í—Ç (–º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö)")
    
    # –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    features.update({
        'is_holiday': 0,
        'is_working_day': int(prediction_datetime.weekday() not in [5, 6]),
        'is_weekend': int(prediction_datetime.weekday() in [5, 6]),
        'is_weekend_or_holiday': int(prediction_datetime.weekday() in [5, 6]),
    })
    print(f"–ö–∞–ª–µ–Ω–¥–∞—Ä—å: –≤—ã—Ö–æ–¥–Ω–æ–π={features['is_weekend']}, —Ä–∞–±–æ—á–∏–π_–¥–µ–Ω—å={features['is_working_day']}")
    
    return features

def retrospective_test_with_analysis():
    """–†–µ—Ç—Ä–æ—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∏—Å—Ç–æ—Ä–∏–∏"""
    
    print("=== –†–ê–°–®–ò–†–ï–ù–ù–û–ï –†–ï–¢–†–û–°–ü–ï–ö–¢–ò–í–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ===")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = load_model()
    print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    df_historical = pd.read_csv(PROCESSED_DATA_FILE)
    df_historical['datetime'] = pd.to_datetime(df_historical['datetime'])
    df_historical['date_only'] = df_historical['datetime'].dt.date
    print(f"–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(df_historical)} –∑–∞–ø–∏—Å–µ–π")
    print(f"   –ü–µ—Ä–∏–æ–¥: {df_historical['datetime'].min()} - {df_historical['datetime'].max()}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_start = datetime.strptime(TEST_START_DATE, '%Y-%m-%d')
    test_end = test_start + timedelta(days=TEST_DAYS)
    
    print(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å {test_start} –ø–æ {test_end}")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∏—Å—Ç–æ—Ä–∏–∏
    max_history_date = test_start - timedelta(days=HISTORICAL_WINDOW_DAYS)
    training_data = df_historical[
        (df_historical['datetime'] < test_start) & 
        (df_historical['datetime'] >= max_history_date)
    ]
    
    test_data = df_historical[
        (df_historical['datetime'] >= test_start) & 
        (df_historical['datetime'] < test_end)
    ]
    
    print(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(training_data)} –∑–∞–ø–∏—Å–µ–π")
    print(f"–ü–µ—Ä–∏–æ–¥: {training_data['datetime'].min()} - {training_data['datetime'].max()}")
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∞–∫—Å–∏–º—É–º {HISTORICAL_WINDOW_DAYS} –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏")
    print(f"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(test_data)} –∑–∞–ø–∏—Å–µ–π")
    
    if len(test_data) == 0:
        print("–ù–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
        return
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
    predictions = []
    actual_values = []
    prediction_times = []
    feature_importance_tracking = []  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    print(f"\n–ù–ê–ß–ê–õ–û –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô ({len(test_data)} —Ç–æ—á–µ–∫)...")
    
    # –ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º
    first_prediction = True
    
    for idx, row in test_data.iterrows():
        prediction_time = row['datetime']
        actual_consumption = row['consumption']
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≥–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        weather_data = {
            'temperature': row.get('temperature', 10.0),
            'humidity': row.get('humidity', 70.0),
            'wind_speed': row.get('wind_speed', 3.0),
        }
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if first_prediction:
            print(f"\nüìù –ü–ï–†–í–û–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –° –ü–û–î–†–û–ë–ù–´–ú –ê–ù–ê–õ–ò–ó–û–ú:")
            features = prepare_features_with_explanation(training_data, prediction_time, weather_data)
            first_prediction = False
        else:
            features = prepare_features_with_explanation(training_data, prediction_time, weather_data)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        features_df = pd.DataFrame([features])
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        for col in features_df.columns:
            if features_df[col].isna().any():
                if col in training_data.columns:
                    median_val = training_data[col].median()
                    features_df[col] = features_df[col].fillna(median_val)
                else:
                    features_df[col] = features_df[col].fillna(0)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        try:
            prediction = model.predict(features_df)[0]
            predictions.append(prediction)
            actual_values.append(actual_consumption)
            prediction_times.append(prediction_time)
            
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            feature_tracking = {
                'datetime': prediction_time,
                'actual': actual_consumption,
                'predicted': prediction,
                'error': abs(prediction - actual_consumption),
                'lag_1': features.get('consumption_lag_1', 0),
                'temperature': features.get('temperature', 0),
                'hour': features.get('hour', 0)
            }
            feature_importance_tracking.append(feature_tracking)
            
            if len(predictions) <= 5 or len(predictions) % 24 == 0:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –∏ –∫–∞–∂–¥—ã–π –Ω–æ–≤—ã–π –¥–µ–Ω—å
                print(f"   {prediction_time}: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {prediction:.1f}, —Ä–µ–∞–ª—å–Ω–æ {actual_consumption:.1f}, –æ—à–∏–±–∫–∞ {abs(prediction - actual_consumption):.1f}")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ {prediction_time}: {e}")
            continue
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    if len(predictions) > 0:
        from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
        
        mae = mean_absolute_error(actual_values, predictions)
        mape = mean_absolute_percentage_error(actual_values, predictions) * 100
        
        print(f"\n" + "="*60)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
        print("="*60)
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(predictions)}")
        print(f"MAE: {mae:.3f}")
        print(f"MAPE: {mape:.2f}%")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ: {np.mean(actual_values):.1f} –ú–í—Ç")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å (¬±2.5% –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ): {mae < (np.mean(actual_values) * 0.025)}")
        print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏: {100 - mape:.1f}%")
        
        # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
        errors = np.abs(np.array(actual_values) - np.array(predictions))
        print(f"\nüìâ –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö:")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {np.min(errors):.1f} –ú–í—Ç")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {np.max(errors):.1f} –ú–í—Ç")
        print(f"   –ú–µ–¥–∏–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {np.median(errors):.1f} –ú–í—Ç")
        print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫: {np.std(errors):.1f} –ú–í—Ç")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        plt.figure(figsize=(20, 12))
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        plt.subplot(3, 2, 1)
        plt.plot(prediction_times, actual_values, 'b-', label='–†–µ–∞–ª—å–Ω–æ–µ', linewidth=2, alpha=0.8)
        plt.plot(prediction_times, predictions, 'r--', label='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ', linewidth=2, alpha=0.8)
        plt.title(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\nMAE={mae:.3f}, MAPE={mape:.2f}%')
        plt.ylabel('–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ (–ú–í—Ç)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏
        plt.subplot(3, 2, 2)
        plt.plot(prediction_times, errors, 'g-', alpha=0.7)
        plt.title(f'–ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏ (—Å—Ä–µ–¥–Ω—è—è = {np.mean(errors):.1f} –ú–í—Ç)')
        plt.ylabel('–û—à–∏–±–∫–∞ (–ú–í—Ç)')
        plt.xlabel('–í—Ä–µ–º—è')
        plt.grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: Scatter plot
        plt.subplot(3, 2, 3)
        plt.scatter(actual_values, predictions, alpha=0.6)
        min_val = min(min(actual_values), min(predictions))
        max_val = max(max(actual_values), max(predictions))
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)
        plt.xlabel('–†–µ–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ')
        plt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ')
        plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π')
        plt.grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –û—à–∏–±–∫–∏ –ø–æ —á–∞—Å–∞–º
        plt.subplot(3, 2, 4)
        df_errors = pd.DataFrame({
            'hour': [t.hour for t in prediction_times],
            'error': errors
        })
        hourly_errors = df_errors.groupby('hour')['error'].mean()
        plt.bar(hourly_errors.index, hourly_errors.values, alpha=0.7)
        plt.xlabel('–ß–∞—Å —Å—É—Ç–æ–∫')
        plt.ylabel('–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ (–ú–í—Ç)')
        plt.title('–°—Ä–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏ –ø–æ —á–∞—Å–∞–º —Å—É—Ç–æ–∫')
        plt.grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 5: –ê–Ω–∞–ª–∏–∑ –ª–∞–≥–æ–≤
        plt.subplot(3, 2, 5)
        lag_errors = pd.DataFrame(feature_importance_tracking)
        plt.scatter(lag_errors['lag_1'], lag_errors['actual'], alpha=0.5, label='–†–µ–∞–ª—å–Ω–æ–µ')
        plt.scatter(lag_errors['lag_1'], lag_errors['predicted'], alpha=0.5, label='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ')
        plt.xlabel('–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ 1 —á–∞—Å –Ω–∞–∑–∞–¥ (–ú–í—Ç)')
        plt.ylabel('–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ (–ú–í—Ç)')
        plt.title('–°–≤—è–∑—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —á–∞—Å–æ–º')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 6: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ vs –û—à–∏–±–∫–∞
        plt.subplot(3, 2, 6)
        plt.scatter(lag_errors['temperature'], lag_errors['error'], alpha=0.5)
        plt.xlabel('–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (¬∞C)')
        plt.ylabel('–û—à–∏–±–∫–∞ (–ú–í—Ç)')
        plt.title('–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—à–∏–±–∫–∏ –æ—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('enhanced_test_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_df = pd.DataFrame({
            'datetime': prediction_times,
            'actual_consumption': actual_values,
            'predicted_consumption': predictions,
            'absolute_error': errors
        })
        results_df.to_csv('enhanced_test_detailed_results.csv', index=False)
        print(f"\n–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ enhanced_test_detailed_results.csv")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–Ω—è–º
        print(f"\n–ê–ù–ê–õ–ò–ó –ü–û –î–ù–Ø–ú:")
        df_daily = pd.DataFrame({
            'datetime': prediction_times,
            'actual': actual_values,
            'predicted': predictions,
            'error': errors
        })
        df_daily['date'] = [t.date() for t in df_daily['datetime']]
        
        daily_stats = df_daily.groupby('date').agg({
            'actual': ['mean', 'std'],
            'predicted': 'mean',
            'error': ['mean', 'std']
        }).round(2)
        
        print("–î–Ω–µ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(daily_stats)
        
    else:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è!")


if __name__ == "__main__":
    # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    retrospective_test_with_analysis()
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    explain_improvements_and_recommendations()
    
    print(f"\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
