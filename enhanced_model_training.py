import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# === –ö–û–ù–°–¢–ê–ù–¢–´ ===
FILTERED_DATA_FILE = 'enhanced_filtered_training_data.csv'
MODEL_PATH = 'enhanced_energy_model.cbm'
FEATURES_IMPORTANCE_PATH = 'enhanced_features_importance.png'

def mape_scorer(y_true, y_pred):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π scorer –¥–ª—è MAPE"""
    return -mean_absolute_percentage_error(y_true, y_pred)  # GridSearch –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç, –ø–æ—ç—Ç–æ–º—É –º–∏–Ω—É—Å

def train_model_with_optimization():
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞–º–∏"""
    print("=== –û–ë–£–ß–ï–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò –° –ü–û–î–ë–û–†–û–ú –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í ===")
    df = pd.read_csv(FILTERED_DATA_FILE)
    df['datetime'] = pd.to_datetime(df['datetime'])
    df = df.sort_values('datetime')

    print(f"–í—Å–µ–≥–æ –¥–∞–Ω–Ω—ã—Ö: {len(df)}")
    print(f"–ü–µ—Ä–∏–æ–¥: {df['datetime'].min()} - {df['datetime'].max()}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    base_features = [
        'hour', 'hour_sin', 'hour_cos', 'dayofweek', 'month', 'week_of_year',
        'temperature', 'humidity', 'wind_speed',
        'is_holiday', 'is_working_weekend', 'is_regular_weekend', 'is_working_day',
        'is_weekend_or_holiday', 'is_weekend',
        'is_winter', 'is_spring', 'is_summer', 'is_autumn'
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ª–∞–≥–∏, —Å–∫–æ–ª—å–∑—è—â–∏–µ –∏ EWM –ø—Ä–∏–∑–Ω–∞–∫–∏
    lag_cols = [col for col in df.columns if 'consumption_lag_' in col]
    rolling_cols = [col for col in df.columns if 'consumption_rolling_' in col]
    ewm_cols = [col for col in df.columns if 'consumption_ewm_' in col]
    
    feature_columns = base_features + lag_cols + rolling_cols + ewm_cols

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    missing_cols = [col for col in feature_columns if col not in df.columns]
    if missing_cols:
        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –¥–∞–Ω–Ω—ã—Ö: {missing_cols}")
        feature_columns = [col for col in feature_columns if col in df.columns]

    X = df[feature_columns]
    y = df['consumption']

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    categorical_features = [
        'hour', 'dayofweek', 'month', 'week_of_year',
        'is_holiday', 'is_working_weekend', 'is_regular_weekend', 'is_working_day',
        'is_weekend_or_holiday', 'is_weekend',
        'is_winter', 'is_spring', 'is_summer', 'is_autumn'
    ]
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    categorical_features = [col for col in categorical_features if col in X.columns]

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    tscv = TimeSeriesSplit(n_splits=5)

    # –°–µ—Ç–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
    param_grid = {
        'iterations': [500],          # –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–∏–º 500, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å [1000]
        'learning_rate': [0.05, 0.1],
        'depth': [6, 8],
        'l2_leaf_reg': [3, 5],
        'random_seed': [42]
    }

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = CatBoostRegressor(
        loss_function='MAPE',
        cat_features=categorical_features,
        verbose=False
    )

    # –ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ü–†–û–ì–†–ï–°–°-–ë–ê–†–û–ú (verbose=2)
    print("\n–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
    print("–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=make_scorer(mape_scorer, greater_is_better=False),
        cv=tscv,
        n_jobs=-1,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞ CPU
        verbose=2   # <-- –ü–†–û–ì–†–ï–°–°-–ë–ê–†! –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    )

    grid_search.fit(X, y)

    print(f"\n–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
    print(f"üìà –õ—É—á—à–∏–π MAPE –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {-grid_search.best_score_:.4f}")

    # –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö —Å –ü–†–û–ì–†–ï–°–°-–ë–ê–†–û–ú
    print("\n–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
    best_model = CatBoostRegressor(
        **grid_search.best_params_,
        loss_function='MAPE',
        cat_features=categorical_features,
        early_stopping_rounds=30,  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        use_best_model=True,       # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—É—á—à—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é
        verbose=50                 # <-- –ü–†–û–ì–†–ï–°–°-–ë–ê–†! –í—ã–≤–æ–¥ –∫–∞–∂–¥—ã–µ 50 –∏—Ç–µ—Ä–∞—Ü–∏–π
    )

    best_model.fit(X, y)

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (–¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
    y_pred_full = best_model.predict(X)
    mae_full = mean_absolute_error(y, y_pred_full)
    mape_full = mean_absolute_percentage_error(y, y_pred_full) * 100
    print(f"\n–û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ–º –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ:")
    print(f"MAE: {mae_full:.3f}")
    print(f"MAPE: {mape_full:.2f}%")

    # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä, –Ω–æ –∑–¥–µ—Å—å –µ–≥–æ –Ω–µ—Ç)
    # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∏–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print(f"\n=== –¢–û–ü-15 –°–ê–ú–´–• –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í ===")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.get_feature_importance()
    }).sort_values('importance', ascending=False)
    
    for i, row in feature_importance.head(15).iterrows():
        print(f"{i+1:2d}. {row['feature']:<30} : {row['importance']:>8.2f}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    best_model.save_model(MODEL_PATH)
    print(f"\n–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(10, 8))
        top_features = feature_importance.head(15)
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
        plt.title('–¢–æ–ø-15 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig(FEATURES_IMPORTANCE_PATH, dpi=300, bbox_inches='tight')
        print(f"–ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {FEATURES_IMPORTANCE_PATH}")
    except Exception as e:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    return best_model, feature_columns

if __name__ == "__main__":
    model, features = train_model_with_optimization()
    print("\n–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
