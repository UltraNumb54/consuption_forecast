import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# === –ö–û–ù–°–¢–ê–ù–¢–´ ===
FILTERED_DATA_FILE = 'enhanced_filtered_training_data.csv'
MODEL_PATH = 'enhanced_energy_model.cbm'
FEATURES_IMPORTANCE_PATH = 'enhanced_features_importance.png'

def mape_scorer(y_true, y_pred):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π scorer –¥–ª—è MAPE"""
    return -mean_absolute_percentage_error(y_true, y_pred)  # GridSearch –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç, –ø–æ—ç—Ç–æ–º—É –º–∏–Ω—É—Å

def train_model_with_optimization():
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞–º–∏"""
    print("=== –û–ë–£–ß–ï–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò –° –ü–û–î–ë–û–†–û–ú –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í ===")
    df = pd.read_csv(FILTERED_DATA_FILE)
    df['datetime'] = pd.to_datetime(df['datetime'])
    df = df.sort_values('datetime')

    print(f"–í—Å–µ–≥–æ –¥–∞–Ω–Ω—ã—Ö: {len(df)}")
    print(f"–ü–µ—Ä–∏–æ–¥: {df['datetime'].min()} - {df['datetime'].max()}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    base_features = [
        'hour', 'hour_sin', 'hour_cos', 'dayofweek', 'month', 'week_of_year',
        'temperature', 'humidity', 'wind_speed',
        'is_holiday', 'is_working_weekend', 'is_regular_weekend', 'is_working_day',
        'is_weekend_or_holiday', 'is_weekend',
        'is_winter', 'is_spring', 'is_summer', 'is_autumn'
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ª–∞–≥–∏, —Å–∫–æ–ª—å–∑—è—â–∏–µ –∏ EWM –ø—Ä–∏–∑–Ω–∞–∫–∏
    lag_cols = [col for col in df.columns if 'consumption_lag_' in col]
    rolling_cols = [col for col in df.columns if 'consumption_rolling_' in col]
    ewm_cols = [col for col in df.columns if 'consumption_ewm_' in col]
    
    feature_columns = base_features + lag_cols + rolling_cols + ewm_cols

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    missing_cols = [col for col in feature_columns if col not in df.columns]
    if missing_cols:
        print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –¥–∞–Ω–Ω—ã—Ö: {missing_cols}")
        feature_columns = [col for col in feature_columns if col in df.columns]

    X = df[feature_columns]
    y = df['consumption']

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    categorical_features = [
        'hour', 'dayofweek', 'month', 'week_of_year',
        'is_holiday', 'is_working_weekend', 'is_regular_weekend', 'is_working_day',
        'is_weekend_or_holiday', 'is_weekend',
        'is_winter', 'is_spring', 'is_summer', 'is_autumn'
    ]
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    categorical_features = [col for col in categorical_features if col in X.columns]

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    tscv = TimeSeriesSplit(n_splits=5)

    # –°–µ—Ç–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
    param_grid = {
        'iterations': [500],          # –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–∏–º 500
        'learning_rate': [0.05, 0.1],
        'depth': [6, 8],
        'l2_leaf_reg': [3, 5],
        'random_seed': [42]
    }

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = CatBoostRegressor(
        loss_function='MAPE',
        cat_features=categorical_features,
        verbose=False
    )

    # –ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ü–†–û–ì–†–ï–°–°-–ë–ê–†–û–ú (verbose=2)
    print("\nüîç –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
    print("–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=make_scorer(mape_scorer, greater_is_better=False),
        cv=tscv,
        n_jobs=-1,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞ CPU
        verbose=2   # <-- –ü–†–û–ì–†–ï–°–°-–ë–ê–†! –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    )

    grid_search.fit(X, y)

    print(f"\nüèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: -grid_search.best_score_ –¥–∞—Å—Ç –ü–û–õ–û–ñ–ò–¢–ï–õ–¨–ù–´–ô MAPE
    print(f"üìà –õ—É—á—à–∏–π MAPE –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {-grid_search.best_score_:.4f} (–∏–ª–∏ {(-grid_search.best_score_)*100:.2f}%)")

    # ===== –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–®–ò–ë–ö–ò use_best_model =====
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ train/val (80/20) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å early stopping
    split_idx = int(len(X) * 0.8)
    X_train_final = X.iloc[:split_idx]
    y_train_final = y.iloc[:split_idx]
    X_val_final = X.iloc[split_idx:]
    y_val_final = y.iloc[split_idx:]

    # –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö —Å –ü–†–û–ì–†–ï–°–°-–ë–ê–†–û–ú
    print("\nüöÄ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
    best_model = CatBoostRegressor(
        **grid_search.best_params_,
        loss_function='MAPE',
        eval_metric='MAPE',  # –î–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–µ
        cat_features=categorical_features,
        early_stopping_rounds=30,  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        use_best_model=True,       # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—É—á—à—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é
        verbose=50                 # <-- –ü–†–û–ì–†–ï–°–°-–ë–ê–†! –í—ã–≤–æ–¥ –∫–∞–∂–¥—ã–µ 50 –∏—Ç–µ—Ä–∞—Ü–∏–π
    )

    # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø–µ—Ä–µ–¥–∞–µ–º eval_set!
    best_model.fit(
        X_train_final, y_train_final,
        eval_set=(X_val_final, y_val_final)  # <-- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï!
    )
    # =============================================

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (–¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
    y_pred_full = best_model.predict(X)
    mae_full = mean_absolute_error(y, y_pred_full)
    mape_full = mean_absolute_percentage_error(y, y_pred_full) * 100
    print(f"\nüìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ–º –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ:")
    print(f"   MAE: {mae_full:.3f}")
    print(f"   MAPE: {mape_full:.2f}%")

    # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (—Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ—à–∏–±–∫—É –Ω–∞ train –∏ val)
    y_pred_train = best_model.predict(X_train_final)
    y_pred_val = best_model.predict(X_val_final)
    mae_train = mean_absolute_error(y_train_final, y_pred_train)
    mae_val = mean_absolute_error(y_val_final, y_pred_val)
    overfitting_ratio = mae_val / mae_train if mae_train > 0 else float('inf')
    print(f"\n‚ö†Ô∏è  –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (MAE val / MAE train): {overfitting_ratio:.2f}")

    # –í—ã–≤–æ–¥ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print(f"\n=== –¢–û–ü-15 –°–ê–ú–´–• –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í ===")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.get_feature_importance()
    }).sort_values('importance', ascending=False)
    
    for i, row in feature_importance.head(15).iterrows():
        print(f"{i+1:2d}. {row['feature']:<30} : {row['importance']:>8.2f}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    best_model.save_model(MODEL_PATH)
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(10, 8))
        top_features = feature_importance.head(15)
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
        plt.title('–¢–æ–ø-15 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig(FEATURES_IMPORTANCE_PATH, dpi=300, bbox_inches='tight')
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {FEATURES_IMPORTANCE_PATH}")
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    return best_model, feature_columns

if __name__ == "__main__":
    model, features = train_model_with_optimization()
    print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
